{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLpyuP5Dft/1JtjzICVJmN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","\n","# Download the complete 'averaged_perceptron_tagger' for English using nltk.download\n","nltk.download('averaged_perceptron_tagger_eng') # Download for english language\n","nltk.download('punkt_tab')\n","# Sample sentence\n","sentence = \"Inbavel quickly developed a Tamil speech-to-text application that improves user experience. \"\n","\n","# Tokenize the sentence\n","tokens = word_tokenize(sentence)\n","\n","# Perform Part-of-Speech tagging\n","tagged_tokens = pos_tag(tokens)\n","\n","# Extract Nouns, Verbs, and Adverbs\n","nouns = [word for word, pos in tagged_tokens if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n","verbs = [word for word, pos in tagged_tokens if pos in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]\n","adverbs = [word for word, pos in tagged_tokens if pos in ['RB', 'RBR', 'RBS']]\n","\n","print(\"Nouns:\", nouns)\n","print(\"Verbs:\", verbs)\n","print(\"Adverbs:\", adverbs)"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BxD-Jr9C847Y","executionInfo":{"status":"ok","timestamp":1737422951001,"user_tz":-330,"elapsed":503,"user":{"displayName":"INBAVEL S","userId":"01930885020195414268"}},"outputId":"4d6a6380-5b13-40b5-cdb7-bad2df2d69f3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Nouns: ['Inbavel', 'Tamil', 'application', 'experience']\n","Verbs: ['developed', 'improves']\n","Adverbs: ['quickly']\n"]}]},{"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","\n","# Download the complete 'averaged_perceptron_tagger' for English using nltk.download\n","nltk.download('averaged_perceptron_tagger_eng')  # Download for English language\n","nltk.download('universal_tagset')  # Download universal tagset\n","\n","# Sample sentence\n","sentence = \"w is a word\"\n","\n","# Tokenize the sentence\n","tokens = word_tokenize(sentence)\n","\n","# Perform Part-of-Speech tagging\n","tagged_tokens = pos_tag(tokens, tagset='universal')  # Use universal tagset\n","\n","# Extract all sentence components using universal tagset\n","components = {\n","    'Nouns': [word for word, pos in tagged_tokens if pos == 'NOUN'],\n","    'Verbs': [word for word, pos in tagged_tokens if pos == 'VERB'],\n","    'Adjectives': [word for word, pos in tagged_tokens if pos == 'ADJ'],\n","    'Adverbs': [word for word, pos in tagged_tokens if pos == 'ADV'],\n","    'Pronouns': [word for word, pos in tagged_tokens if pos == 'PRON'],\n","    'Determiners': [word for word, pos in tagged_tokens if pos == 'DET'],\n","    'Adpositions': [word for word, pos in tagged_tokens if pos == 'ADP'],\n","    'Conjunctions': [word for word, pos in tagged_tokens if pos == 'CONJ'],\n","    'Numerals': [word for word, pos in tagged_tokens if pos == 'NUM'],\n","    'Particles': [word for word, pos in tagged_tokens if pos == 'PRT'],\n","    '.' : [word for word, pos in tagged_tokens if pos == '.'],\n","    'X': [word for word, pos in tagged_tokens if pos == 'X']\n","}\n","\n","# Print the extracted components\n","for component_type, component_words in components.items():\n","    print(f\"{component_type}: {component_words}\")"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z_b0W1IC-9IR","executionInfo":{"status":"ok","timestamp":1737422959863,"user_tz":-330,"elapsed":449,"user":{"displayName":"INBAVEL S","userId":"01930885020195414268"}},"outputId":"e447e660-b88d-4e13-ac7e-87e9bc783a51"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Nouns: ['w', 'word']\n","Verbs: ['is']\n","Adjectives: []\n","Adverbs: []\n","Pronouns: []\n","Determiners: ['a']\n","Adpositions: []\n","Conjunctions: []\n","Numerals: []\n","Particles: []\n",".: []\n","X: []\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"]}]},{"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","\n","# Sample text\n","text = \"This is a test sentence. This sentence is a test.\"\n","\n","# Tokenize the text\n","tokens = word_tokenize(text)\n","\n","# Count word frequencies\n","word_counts = Counter(tokens)\n","\n","# Print repeated words and their counts\n","for word, count in word_counts.items():\n","  if count > 1:\n","    print(f\"'{word}': {count}\")"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v7uCpFcBAbVQ","executionInfo":{"status":"ok","timestamp":1737422969381,"user_tz":-330,"elapsed":1449,"user":{"displayName":"INBAVEL S","userId":"01930885020195414268"}},"outputId":"273ac7db-f029-4941-f5c8-cd5195f74c0b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["'This': 2\n","'is': 2\n","'a': 2\n","'test': 2\n","'sentence': 2\n","'.': 2\n"]}]},{"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')  # Download necessary data for tokenization\n","\n","sentence = \"This is a test sentence.\"  # Your sentence\n","\n","# Tokenize the sentence\n","tokens = word_tokenize(sentence)\n","\n","# Get the word count\n","word_count = len(tokens)\n","\n","print(f\"The sentence has {word_count} words.\")"],"cell_type":"code","metadata":{"id":"g2D2wXhOAwY3","executionInfo":{"status":"ok","timestamp":1737422975676,"user_tz":-330,"elapsed":1147,"user":{"displayName":"INBAVEL S","userId":"01930885020195414268"}},"outputId":"48c4935e-3504-4fb9-c899-da1f611c228b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["The sentence has 6 words.\n"]}]},{"cell_type":"code","source":["pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zziSofRu6CQk","executionInfo":{"status":"ok","timestamp":1737422902398,"user_tz":-330,"elapsed":3622,"user":{"displayName":"INBAVEL S","userId":"01930885020195414268"}},"outputId":"162bf7ef-18d2-44f5-f0eb-359b791afa77"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]}]}]}